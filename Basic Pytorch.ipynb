{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "import numpy as np\n",
    "from go_ai import data\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "BOARD_SIZE = 5\n",
    "MODEL_SAVE_FILE = 'models/actorcritic_{0}x{0}.h5'.format(BOARD_SIZE)\n",
    "LOAD_TRAINED = True\n",
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(6 * board_size * board_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(256, board_size * board_size + 1),\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        invalid_values = data.batch_invalid_values(state)\n",
    "        x = torch.flatten(state, start_dim=1)\n",
    "        x = self.main(x)\n",
    "        policy = self.policy(x)\n",
    "        policy += invalid_values\n",
    "        policy = nn.functional.softmax(policy, dim=1)\n",
    "        value = self.value(x)\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, model1, model2):\n",
    "    states = []\n",
    "    \n",
    "    env.reset()\n",
    "    state = env.get_canonical_state()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        turn = env.turn()\n",
    "        state_tensor = torch.from_numpy(state[np.newaxis]).type(torch.FloatTensor)\n",
    "        if turn == 0:\n",
    "            action_probs, _ = model1(state_tensor)\n",
    "        else:\n",
    "            action_probs, _ = model2(state_tensor)\n",
    "        action = np.random.choice(np.arange(0, env.size * env.size + 1), p=action_probs.detach().numpy()[0])\n",
    "        _, _, done, _ = env.step(action)\n",
    "        state = env.get_canonical_state()\n",
    "        states.append(state)\n",
    "    winner = env.get_winner()\n",
    "    canonical_winners = [winner if i % 2 == 0 else 1 - winner for i in range(len(states))]\n",
    "    return states, canonical_winners\n",
    "\n",
    "def generate_trajectories(env, model1, model2, num_episodes):\n",
    "    state_list = []\n",
    "    winner_list = []\n",
    "    pbar = tqdm_notebook(range(num_episodes), desc='Trajectory generation')\n",
    "    for i in pbar:\n",
    "        states, winners = play_game(env, model1, model2)\n",
    "        state_list.extend(states)\n",
    "        winner_list.extend(winners)\n",
    "        pbar.set_postfix_str('Average length: ' + str(len(state_list) / (i + 1)))\n",
    "    return state_list, winner_list\n",
    "\n",
    "def pit(env, model1, model2, num_episodes):\n",
    "    model1_wins = 0\n",
    "    model2_wins = 0\n",
    "    pbar = tqdm_notebook(range(num_episodes // 2), desc='Playing black')\n",
    "    for i in pbar:\n",
    "        _, winners = play_game(env, model1, model2)\n",
    "        if winners[0] == 1:\n",
    "            model1_wins += 1\n",
    "        elif winners[0] == 0:\n",
    "            model2_wins += 1\n",
    "    pbar = tqdm_notebook(range(num_episodes // 2), desc='Playing white')\n",
    "    for i in pbar:\n",
    "        _, winners = play_game(env, model2, model1)\n",
    "        if winners[0] == 1:\n",
    "            model2_wins += 1\n",
    "        elif winners[0] == 0:\n",
    "            model1_wins += 1\n",
    "    print('Model 1 WR: {}'.format(model1_wins / num_episodes))\n",
    "    print('Model 2 WR: {}'.format(model2_wins / num_episodes))\n",
    "    return model1_wins, model2_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(model, opt, states, winners, batch_size):\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    winner_batches = np.array_split(winners, len(winners) // batch_size)\n",
    "    pbar = tqdm_notebook(range(len(state_batches)), desc='Policy evaluation')\n",
    "    for b in pbar:\n",
    "        b_s = torch.from_numpy(state_batches[b]).type(torch.FloatTensor)\n",
    "        b_w = winner_batches[b]\n",
    "        b_w_tensor = torch.from_numpy(b_w).type(torch.FloatTensor)\n",
    "        opt.zero_grad()\n",
    "        _, pred_win = model(b_s)\n",
    "        loss = nn.functional.binary_cross_entropy(pred_win[:,0], b_w_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        correct = (pred_win > 0.5).type(torch.IntTensor)[:,0] == b_w_tensor.type(torch.IntTensor)\n",
    "        accuracy = np.mean(correct.numpy())\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()) + ' Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from go_ai.montecarlo import invert_qval, canonical_winning, batch_canonical_children_states\n",
    "\n",
    "def get_qvals(env, model, states):\n",
    "    canonical_next_states = batch_canonical_children_states(states)\n",
    "    next_states_tensor = torch.from_numpy(canonical_next_states).type(torch.FloatTensor)\n",
    "    _, canonical_next_vals = model(next_states_tensor)\n",
    "\n",
    "    curr_idx = 0\n",
    "    batch_qvals = []\n",
    "    for state in states:\n",
    "        valid_moves = env.gogame.get_valid_moves(state)\n",
    "        Qs = []\n",
    "        for move in range(env.gogame.get_action_size(state)):\n",
    "            if valid_moves[move]:\n",
    "                canonical_next_state = canonical_next_states[curr_idx]\n",
    "                terminal = env.gogame.get_game_ended(canonical_next_state)\n",
    "                winning = canonical_winning(canonical_next_state)\n",
    "                oppo_val = (1 - terminal) * canonical_next_vals[curr_idx].item() + (terminal) * winning\n",
    "                qval = invert_qval(oppo_val)\n",
    "                Qs.append(qval)\n",
    "                curr_idx += 1\n",
    "            else:\n",
    "                Qs.append(0)\n",
    "\n",
    "        batch_qvals.append(Qs)\n",
    "\n",
    "    assert curr_idx == len(canonical_next_vals), (curr_idx, len(canonical_next_vals))\n",
    "    return np.array(batch_qvals)\n",
    "\n",
    "def policy_iter(env, model, opt, states, batch_size):\n",
    "    state_batches = np.array_split(states, len(states) // batch_size)\n",
    "    pbar = tqdm_notebook(state_batches, desc='Policy iteration')\n",
    "    for states in pbar:\n",
    "        states_tensor = torch.from_numpy(states).type(torch.FloatTensor)\n",
    "        policy, _ = model(states_tensor)\n",
    "        qvals = get_qvals(env, model, states)\n",
    "        greedy = np.argmax(qvals, axis=1)\n",
    "        greedy_tensor = torch.from_numpy(greedy).type(torch.LongTensor)\n",
    "        opt.zero_grad()\n",
    "        loss = nn.functional.cross_entropy(policy, greedy_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.set_postfix_str('Loss: ' + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(env, model, opt, batch_size, num_episodes):\n",
    "    states, winners = generate_trajectories(go_env, model, model, num_episodes)\n",
    "    policy_eval(model, opt, states, winners, batch_size)\n",
    "    policy_iter(env, model, opt, states, batch_size)\n",
    "    \n",
    "def train(env, model, iterations, lr, batch_size, eps_per_iter):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(iterations):\n",
    "        train_step(env, model, opt, batch_size, eps_per_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cb975d688345e79fb06a38ff1e8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Trajectory generation', max=1024, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d2456ea5bf48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_per_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_SAVE_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d88073f5ed97>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, model, iterations, lr, batch_size, eps_per_iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_per_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-d88073f5ed97>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(env, model, opt, batch_size, num_episodes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpolicy_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpolicy_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d82f5443d29c>\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[0;34m(env, model1, model2, num_episodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Trajectory generation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mstate_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mwinner_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d82f5443d29c>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, model1, model2)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_canonical_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PolicyValueNet(BOARD_SIZE)\n",
    "if LOAD_TRAINED:\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_FILE))\n",
    "    \n",
    "train(go_env, model, iterations=10, lr=0.001, batch_size=32, eps_per_iter=1024)\n",
    "torch.save(model.state_dict(), MODEL_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861bfe17e8bc4bfb8dd6359f8858338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing black', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ae0bbe779141a7ac49d5d507d599fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing white', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1 WR: 0.502\n",
      "Model 2 WR: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(251, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = PolicyValueNet(5)\n",
    "baseline.load_state_dict(torch.load('models/acbaseline_5x5.h5'))\n",
    "pit(go_env, model, baseline, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941dabce17e748b9b382bc2a6ac0dca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing black', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee495e113e8a431f9e9b95b088c053e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Playing white', max=250, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1 WR: 0.434\n",
      "Model 2 WR: 0.408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(217, 204)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pit(go_env, model, model, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
