{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88zLHqvDwJRj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from go_ai import data, metrics, mcts, models, policies\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUqQzSilGa7K"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "logging._warn_preinit_stderr = 0\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfY-_1_5wJR5"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ju4HYnKwJRq"
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DX04C6TbwJR5"
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 256\n",
    "EPISODES_PER_ITERATION = 32\n",
    "MAX_STEPS = 2 * BOARD_SIZE**2\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lECYa5ggGa7W"
   },
   "outputs": [],
   "source": [
    "NUM_EVAL_GAMES = 32\n",
    "ITERATIONS_PER_EVAL = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8pWCj8jGa7Y"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ix9zQ82Ga7a"
   },
   "outputs": [],
   "source": [
    "MC_SIMS = 0\n",
    "TEMP_FUNC = lambda step: 1/(step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfJcqNeEGa7b"
   },
   "outputs": [],
   "source": [
    "WEIGHTS_DIR = 'model_weights/'\n",
    "ACTOR_CRITIC_PATH = WEIGHTS_DIR + 'checkpoint_{}x{}.h5'.format(BOARD_SIZE, BOARD_SIZE)\n",
    "LOAD_SAVED_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyALIMpwJRo"
   },
   "source": [
    "# Go Environment\n",
    "Train on a small board with heuristic reward for fast training and efficient debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4w7gMrfwJRp"
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbI1xPoVGa7e"
   },
   "source": [
    "# Metrics and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc1lqGjPwJRm"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuHFwINdGa7g"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4hPKfRaGa7g"
   },
   "outputs": [],
   "source": [
    "tb_metrics = {}\n",
    "for metric_key in ['val_loss', 'overall_loss', 'num_steps', 'move_loss']:\n",
    "    tb_metrics[metric_key] = tf.keras.metrics.Mean('{}'.format(metric_key), \n",
    "                                                   dtype=tf.float32)\n",
    "tb_metrics['pred_win_acc'] = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNKtplyZGa7h"
   },
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvHrRtTfGa7h"
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/actor_critic/{}/main'.format(current_time)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7ZoRfzIwJRr"
   },
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VDp7tLiGa7j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor_critic = models.make_actor_critic(BOARD_SIZE, 'val_net', 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x7SlLhZGa7l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tf.keras.utils.plot_model(actor_critic, to_file='logs/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "1FsLyPsAGa7m",
    "outputId": "6786c8ef-0958-4c82-d4ac-40cb4ceebb3a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_critic\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "board (InputLayer)              [(None, 7, 7, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 7, 7, 64)     3520        board[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 7, 7, 64)     256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 7, 7, 64)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     36928       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 7, 64)     256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 7, 7, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 2)      130         re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 1)      65          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 2)      8           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 1)      4           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 7, 7, 2)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 7, 7, 1)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 98)           0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 49)           0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           4950        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "invalid_values (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           2500        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 50)           0           dense[0][0]                      \n",
      "                                                                 invalid_values[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "valid_moves (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "move_probs (Softmax)            (None, 50)           0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            51          re_lu_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 48,668\n",
      "Trainable params: 48,406\n",
      "Non-trainable params: 262\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlpiIb4KGa7p"
   },
   "outputs": [],
   "source": [
    "opponent = tf.keras.models.clone_model(actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4m-C0k6Ga7q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0918 14:21:23.020457 4591560128 <ipython-input-17-1004a61459a1>:4] Loaded saved models\n"
     ]
    }
   ],
   "source": [
    "if LOAD_SAVED_MODELS:\n",
    "    actor_critic.load_weights(ACTOR_CRITIC_PATH)\n",
    "    opponent.load_weights(ACTOR_CRITIC_PATH)\n",
    "    logging.info(\"Loaded saved models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = go_env.get_state()\n",
    "my_policy = policies.MctPolicy(actor_critic, state, MC_SIMS, TEMP_FUNC)\n",
    "opponent_policy = policies.MctPolicy(opponent, state, MC_SIMS, TEMP_FUNC)\n",
    "random_policy = policies.RandomPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVCEKWx_Ga7r"
   },
   "source": [
    "# Demo Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSQ1RFHuGa7r"
   },
   "source": [
    "Symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98jmZoKvwJRv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.plot_symmetries(go_env, actor_critic, 'logs/symmetries.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqVpHyRLGa7s"
   },
   "source": [
    "Plot a whole game trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kkqnueRjGa7t",
    "outputId": "098a59a2-c1a0-4e9b-8344-cd44a8fc8b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 306 ms, total: 11.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "traj, _ = data.self_play(go_env, policy=my_policy, max_steps=MAX_STEPS, \n",
    "                         get_symmetries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrTjrYjBGa7u"
   },
   "outputs": [],
   "source": [
    "fig = metrics.gen_traj_fig(go_env, actor_critic, TEMP_FUNC, MAX_STEPS, \n",
    "                           MC_SIMS)\n",
    "fig.savefig('logs/a_trajectory.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNBj_gKPwJR_"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-oRYYzzGa7v"
   },
   "outputs": [],
   "source": [
    "actor_critic_opt = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "replay_mem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LT1PUXyXwJR_",
    "outputId": "100dda95-316e-457c-b1f3-c8bf82243c52",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Self Play:   6%|â–‹         | 2/32 [00:18<04:25,  8.84s/it]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c7f030ea64bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisode_pbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         trajectory, num_steps = data.self_play(go_env, policy=my_policy, \n\u001b[0;32m----> 8\u001b[0;31m                                                max_steps=MAX_STEPS)\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mreplay_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtb_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/Go-AI/go_ai/data.py\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(go_env, policy, max_steps, get_symmetries)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Get action from MCT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mmcts_action_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcanonical_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgogame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_weighted_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcts_action_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Execute actions in environment and MCT tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/gym-go/gym_go/gogame.py\u001b[0m in \u001b[0;36mrandom_weighted_action\u001b[0;34m(move_weights)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[1;32m    252\u001b[0m         \u001b[0mmove_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmove_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "for iteration in range(ITERATIONS):\n",
    "    # Train\n",
    "    episode_pbar = tqdm(range(EPISODES_PER_ITERATION), \n",
    "                        desc='Iteration {} - Self Play'.format(iteration), \n",
    "                        leave=True, position=0)\n",
    "    for episode in episode_pbar:\n",
    "        trajectory, num_steps = data.self_play(go_env, policy=my_policy, \n",
    "                                               max_steps=MAX_STEPS)\n",
    "        replay_mem.extend(trajectory)\n",
    "        tb_metrics['num_steps'].update_state(num_steps)\n",
    "        \n",
    "    # Update the models (also shuffles memory)\n",
    "    random.shuffle(replay_mem)\n",
    "    np_data = data.replay_mem_to_numpy(replay_mem)\n",
    "    batched_np_data = [np.array_split(datum, len(replay_mem) // BATCH_SIZE) \n",
    "                       for datum in np_data]\n",
    "    batched_mem = list(zip(*batched_np_data))\n",
    "    models.update_win_prediction(actor_critic, batched_mem, actor_critic_opt, \n",
    "                                 iteration, tb_metrics)\n",
    "    \n",
    "    # Evaluate against previous model\n",
    "    if (iteration+1) % ITERATIONS_PER_EVAL == 0:\n",
    "        rand_win_rate = metrics.evaluate(go_env, my_policy, random_policy,\n",
    "                                                 max_steps=MAX_STEPS, \n",
    "                                                 num_games=8)\n",
    "        opp_win_rate = metrics.evaluate(go_env, my_policy, opponent_policy, \n",
    "                                    max_steps=MAX_STEPS, \n",
    "                                    num_games=NUM_EVAL_GAMES)\n",
    "        \n",
    "        stats = \"Opp. - {:.1f}%, Rand. - {:.1f}%\".format(100*rand_win_rate, \n",
    "                                                         100*opp_win_rate)\n",
    "        if win_rate > 0.6:\n",
    "            actor_critic.save_weights(ACTOR_CRITIC_PATH)\n",
    "            opponent.load_weights(ACTOR_CRITIC_PATH)\n",
    "            \n",
    "            logging.info(\"{} Accepted new model\".format(stats))\n",
    "        else:\n",
    "            logging.info(\"{} Rejected new model\".format(stats))\n",
    "    \n",
    "    # Log results and resets the metrics\n",
    "    \n",
    "    metrics.log_to_tensorboard(summary_writer, tb_metrics, iteration, go_env, \n",
    "                               actor_critic, TEMP_FUNC, 'logs/a_trajectory.png')\n",
    "    # Reset memory\n",
    "    replay_mem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyMNxMAWwJR_"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Moy9uJ6fGa7z"
   },
   "source": [
    "Play against our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97j_uYY9wJSA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=BOARD_SIZE)\n",
    "state = go_env.reset()\n",
    "opponent_policy = policies.MctPolicy(network, state, MC_SIMS, TEMP_FUNC)\n",
    "data.play_against(opponent_policy, go_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emKESg3hGa71"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of go_ai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
